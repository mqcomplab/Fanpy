====================================
 Keras Network Wavefunction Example
====================================

The following scripts are generated by using :func:`fanpy_make_script
<script_make_script>` and by tweaking the generated script. For more information
on using :func:`fanpy_make_script <script_make_script>`, go to :ref:`Using a
script that makes a script <tutorial_calc_make_script>` for the tutorial and :ref:`fanpy_make_script
<script_make_script>` for the API. For more information on
customizing the script, go to :ref:`How to run a calculation by making a script
<tutorial_calc_code>`.

For more information, see :py:class:`KerasNetwork <fanpy.wfn.network.keras_network.KerasNetwork>`.

KerasNetwork Configuration
--------------------------

Wavefunction
   KerasNetwork
Hamiltonian
   Restricted Molecular Hamiltonian
Optimized Parameters
   Orbitals are optimized
   KerasNetwork weights are optimized
Objective
   Energy calculated using all Slater determinants
Optimizer
   CMA solver

.. code:: python

    import numpy as np
    import os
    import sys
    from fanpy.wfn.network.keras_network import KerasNetwork
    from fanpy.ham.restricted_chemical import RestrictedMolecularHamiltonian
    from fanpy.tools.sd_list import sd_list
    from fanpy.eqn.least_squares import LeastSquaresEquations
    from fanpy.solver.equation import cma


    # Number of electrons
    nelec = 4
    print('Number of Electrons: {}'.format(nelec))

    # One-electron integrals
    one_int_file = 'oneint.npy'
    one_int = np.load(one_int_file)
    print('One-Electron Integrals: {}'.format(os.path.abspath(one_int_file)))

    # Two-electron integrals
    two_int_file = 'twoint.npy'
    two_int = np.load(two_int_file)
    print('Two-Electron Integrals: {}'.format(os.path.abspath(two_int_file)))

    # Number of spin orbitals
    nspin = one_int.shape[0] * 2
    print('Number of Spin Orbitals: {}'.format(nspin))

    # Nuclear-nuclear repulsion
    nuc_nuc = 0.0
    print('Nuclear-nuclear repulsion: {}'.format(nuc_nuc))

    # Initialize wavefunction
    wfn = KerasNetwork(nelec, nspin, model=None, params=None, dtype=None, memory=None)
    print('Wavefunction: MatrixProductState')

    # Initialize Hamiltonian
    ham = RestrictedMolecularHamiltonian(one_int, two_int)
    print('Hamiltonian: RestrictedMolecularHamiltonian')

    # Projection space
    pspace = sd_list(nelec, nspin, num_limit=None, exc_orders=[1, 2], spin=None,
                    seniority=wfn.seniority)
    print('Projection space (orders of excitations): [1, 2]')

    # Select parameters that will be optimized
    param_selection = [(wfn, np.ones(wfn.nparams, dtype=bool)), (ham, np.ones(ham.nparams, dtype=bool))]

    # Initialize objective
    objective = LeastSquaresEquations(wfn, ham, param_selection=param_selection, pspace=pspace,
                                      refwfn=None, energy_type='compute', energy=None, constraints=None,
                                      eqn_weights=None)
    objective.tmpfile = ''

    # Solve
    print('Optimizing wavefunction: cma solver')
    results = cma(objective, sigma0=0.01, options={'ftarget': None, 'timeout': np.inf, 'tolfun': 1e-11,
                  'verb_filenameprefix': 'outcmaes', 'verb_log': 1})

    # Results
    if results['success']:
        print('Optimization was successful')
    else:
        print('Optimization was not successful: {}'.format(results['message']))
    print('Final Electronic Energy: {}'.format(results['energy']))
    print('Final Total Energy: {}'.format(results['energy'] + nuc_nuc))

Different Networks
------------------
The default network used is a feed-forward network with two hidden layers. The input is the
occupation of each of the spin orbitals (`1` or `0`). The number of hidden units for each layer is
the number of spin orbitals. There are no bias by default. All of the activation functions are the
rectified linear unit (ReLU). Apart from the input and the output (overlap of the given Slater
determinant), the network's structure can be modified using the Keras API. To use a different
network, build the desired model (:code:`tensorflow.keras.engine.training.Model`) and assign it to the
wavefunction. For example,

.. code:: python

    model = keras.engine.sequential.Sequential()
    model.add(keras.layers.core.Dense(nspin, activation=keras.activations.relu, input_dim=nspin,
                                      use_bias=True)
    model.add(keras.layers.core.Dense(int(nspin * 0.7), activation=keras.activations.relu,
                                      input_dim=nspin, use_bias=True)
    model.add(keras.layers.core.Dense(int(nspin * 0.7), activation=keras.activations.relu,
                                      input_dim=nspin, use_bias=True)
    model.add(keras.layers.core.Dense(int(nspin / 0.7), activation=keras.activations.relu,
                                      input_dim=nspin, use_bias=True)
    model.add(keras.layers.core.Dense(int(nspin / 0.7), activation=keras.activations.softmax,
                                      input_dim=nspin, use_bias=True)
    model.add(keras.layers.core.Dense(1, activation=keras.activations.linear,
                                      input_dim=nspin, use_bias=True)
    wfn = KerasNetwork(nelec, nspin, model=model, params=None, dtype=None, memory=None)

Please note that the random initial guess commonly used when training neural networks will not be
feasible here because we aim to find a specific eigenstate (e.g. lowest energy). The default initial
guess is created only for multi-layer perceptrons with only one type of weights for a layer (i.e. no
bias) and the number of hidden units in the last hidden layer is suitably larger than the number of
electrons. To elaborate, if we treat the last hidden layer as a set of spin orbitals, the number of
first and second order excitations must be greater than the number of units. If the default initial
guess cannot be generated for the given model, then the user must provide it.

For documentation on Keras, see `Keras Documentation <https://keras.io/>`_.
